# Big CSV Processor ğŸš€

**Memory-efficient CSV to Parquet converter with 94.5% compression ratio**

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Pandas](https://img.shields.io/badge/Pandas-2.0%2B-red)](https://pandas.pydata.org/)
[![PyArrow](https://img.shields.io/badge/PyArrow-Latest-orange)](https://arrow.apache.org/docs/python/)

> Transform massive CSV files into lightning-fast Parquet format without running out of memory!

## ğŸ“Š The Problem

Ever tried to load a multi-gigabyte CSV file with pandas and got this?

```python
df = pd.read_csv("huge_file.csv")  # âŒ MemoryError!
```

**Big CSV Processor** solves this by using intelligent chunked processing and advanced compression techniques.

## âœ¨ Key Features

- ğŸ”¥ **94.5% size reduction** (7.6GB â†’ 0.42GB in real tests)
- âš¡ **10x faster queries** compared to CSV
- ğŸ’¾ **Memory-efficient** chunked processing
- ğŸ” **Built-in search system** for instant data exploration
- ğŸ›¡ï¸ **Production-ready** with error handling
- ğŸ“¦ **ZSTD compression** for maximum efficiency

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yldrykuru/big-csv-processor.git
cd big-csv-processor

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage

```bash
# Convert CSV to Parquet
python main.py

# Search the converted data
python search_parquet.py
```

## ğŸ“ˆ Real-World Results

| Metric | Before (CSV) | After (Parquet) | Improvement |
|--------|--------------|-----------------|-------------|
| **File Size** | 7.6 GB | 0.42 GB | **94.5% reduction** |
| **Memory Usage** | 8+ GB | 1.5 GB | **81% less RAM** |
| **Query Time** | 30+ seconds | <2 seconds | **15x faster** |
| **Storage Cost** | $$$$ | $ | **Massive savings** |

## ğŸ› ï¸ How It Works

### 1. Chunked Processing (`main.py`)

```python
# Process large files in manageable chunks
csv_reader = pd.read_csv(
    csv_file,
    chunksize=50000,  # Optimal chunk size
    dtype=str,        # Prevent schema conflicts
    low_memory=False
)

# Convert each chunk to compressed Parquet
for chunk in csv_reader:
    chunk.to_parquet(temp_file, compression="zstd")
```

### 2. Smart Compression

- **ZSTD compression**: Best-in-class compression ratio
- **Columnar format**: Efficient data storage and querying
- **Schema optimization**: Automatic type inference

### 3. Lightning-Fast Search (`search_parquet.py`)

```bash
# Interactive mode
python search_parquet.py

# Quick searches
python search_parquet.py search column_name "value"
python search_parquet.py contains column_name "partial_text"
python search_parquet.py info  # Show file structure
```

## ğŸ“‹ Requirements

```txt
pandas>=2.0.0
pyarrow>=10.0.0
```

## ğŸ¯ Usage Examples

### Convert Any CSV File

```python
import pandas as pd
from pathlib import Path

# Modify these variables for your file
csv_file = "your_large_file.csv"
parquet_file = "output.parquet"
chunk_size = 50000

# Run the conversion
python main.py
```

### Search Your Data

```python
# Load and explore
python search_parquet.py info

# Find specific records
python search_parquet.py search "State" "California"

# Text search
python search_parquet.py contains "Name" "Smith"
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Large     â”‚â”€â”€â”€â–¶â”‚   Chunked    â”‚â”€â”€â”€â–¶â”‚ Compressed  â”‚
â”‚   CSV       â”‚    â”‚  Processing  â”‚    â”‚  Parquet    â”‚
â”‚   7.6 GB    â”‚    â”‚              â”‚    â”‚   0.42 GB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚    Search    â”‚
                   â”‚    System    â”‚
                   â”‚   <2 sec     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Performance Benchmarks

### File Size Comparison
- **Original CSV**: 7.6 GB
- **Parquet (uncompressed)**: 2.1 GB (72% reduction)
- **Parquet + GZIP**: 0.8 GB (89% reduction)
- **Parquet + ZSTD**: 0.42 GB (94.5% reduction) â­

### Query Performance
- **CSV + pandas**: 30+ seconds
- **Parquet + pandas**: 8 seconds

## ğŸ”§ Configuration

### Chunk Size Optimization

| RAM Available | Recommended Chunk Size |
|---------------|------------------------|
| 4 GB | 25,000 rows |
| 8 GB | 50,000 rows (default) |
| 16 GB+ | 100,000 rows |

### Compression Options

```python
# Available compression algorithms
compression_options = [
    "zstd",    # Best ratio (recommended)
    "gzip",    # Good balance
    "snappy",  # Fastest
    "lz4",     # Fast with decent ratio
]
```

## ğŸš€ Production Deployment

### Enterprise File Structure

```
/data/
â”œâ”€â”€ raw/                 # Original CSV files
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ 2023/
â”‚   â”‚   â”œâ”€â”€ 01/         # Monthly partitions
â”‚   â”‚   â””â”€â”€ 02/
â”‚   â””â”€â”€ 2024/
â”œâ”€â”€ indexes/
â”‚   â”œâ”€â”€ metadata.sqlite # File metadata
â”‚   â””â”€â”€ search_cache.db # Query cache
â””â”€â”€ backups/            # Compressed backups
```

### Docker Support

```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main.py"]
```

## ğŸ¤ Contributing

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **pandas** team for the amazing data processing library
- **Apache Arrow** team for the high-performance Parquet implementation

## ğŸ“š Related Articles

- [Medium Article: From 7.6GB CSV to 0.42GB Parquet]([https://medium.com/@username/article-link](https://yldrykuru.medium.com/from-7-6gb-csv-to-0-42gb-parquet-production-ready-big-data-solution-d09dc1f1bbd8))

## ğŸ”— Useful Links

- [Parquet Documentation](https://parquet.apache.org/docs/)
- [pandas Documentation](https://pandas.pydata.org/docs/)
- [PyArrow Documentation](https://arrow.apache.org/docs/python/)

## ğŸ“Š Project Stats

![Project Stats](https://github-readme-stats.vercel.app/api/pin/?username=yldrykuru&repo=big-csv-processor&theme=dark)

---

â­ **Star this repo if it helped you!** â­

**Made with â¤ï¸ for the data science community** 
